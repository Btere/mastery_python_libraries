{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/path/to/dataset.xlsx\", index_col=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating custom dataframe\n",
    "\n",
    "new_dataset = {\"Q5\": [234,567,879,4,56.6, 765.0,876.4, 234.7, 098.5, 456.9,342,908, 432,765, 123.6, 798.9, 43.6, \"big\", \"large\"], \"Q4\": [234, 567, 879, 4, 56.6,765.0, 876.4, 234.7,98.5,456.9, 342, 908, 432, 765, 123.6, 798.9, 43.6, \"big\", \"large\"]}\n",
    "df2 = pd.DataFrame(new_dataset)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We want to merge the first dataset, with the second, using the merge function. However, if your new DataFrame is an extension of the existing DataFrame (i.e., you want to add new rows or columns that do not share a common key), you can use the \"concat function\" from pandas instead of \"merge\". The concat function allows you to concatenate DataFrames along a particular axis (either rows or columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = pd.concat([df, df2], axis=1) #axis = 1, say,joined at the column\n",
    "join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.sort_values(\"Q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join_df[\"Q3\"]  to grab a column, we use this, but if it is a single word(column_name) we can use below.\n",
    "join_df.Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the row and col using slicing method\n",
    "accessin_data = join_df.loc[0:4] #loc[row, col]--- loc used label based or slicing\n",
    "accessin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing row only by index label and loc:\n",
    "row = join_df.loc[5]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing multiple rows by index labels and loc\n",
    "rows = join_df.loc[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing rows based on a condition:\n",
    "\n",
    "rows = join_df.loc[join_df['Q3'] > 200]\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using .iloc (Integer-location based indexing)\n",
    "The .iloc method is used to access rows and columns by integer position.\n",
    "\n",
    "Accessing a single row by integer index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = join_df.iloc[0]  #This returns a Series representing the row\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = join_df.iloc[[0, 1, 2]]\n",
    "print(rows)  #seeing the output using the print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = join_df.iloc[[0, 1, 2]]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = join_df.loc[:, :\"Q5\"] #loc is selecting row and col based on column_name or label.\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing a range of rows by integer indices\n",
    "rows = join_df.iloc[0:7, [3, 4]]\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Use join_df Twice?\n",
    "1. First join_df: This refers to the entire DataFrame on which you want to apply the condition.\n",
    "2. Second join_df: This is within the boolean indexing operation where the boolean Series is used to filter the rows of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = join_df[join_df['Q3'] > 345.6]\n",
    "row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join_df: This is the DataFrame that you are working with.\n",
    "\n",
    "join_df['Q2'] > 4: This condition checks each value in the column Q2 to see if it is greater than 4. It returns a Series of Boolean values (True or False), where each entry corresponds to whether the condition is met for each row.\n",
    "\n",
    "join_df['Q3'] < 876.4: This condition checks each value in the column Q3 to see if it is less than 876.4. Like the previous condition, it returns a Series of Boolean values.\n",
    "\n",
    "&: This is the logical AND operator, which combines the two conditions. It returns True only if both conditions are True for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = join_df[(join_df['Q2'] > 4) & (join_df['Q3'] < 876.4)]\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access a single column by using the column name inside square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df['Q3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access multiple col, you pass the column name into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_df_col = join_df[[\"Q1\", \"Q4\", \"Q5\"]]\n",
    "access_df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_df_col = join_df[0:4] #slicing is used for  both row and col simultaneously\n",
    "access_df_col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_df_col = join_df[-5:] #slicing is used for  both row and col simultaneously\n",
    "access_df_col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[:, 'column_name']  # Access a single column\n",
    "#df.loc[:, ['col1', 'col2']]  # Access multiple columns, using the column_name(label_based)\n",
    "\n",
    "join_df.loc[:, [\"Q1\", \"Q4\", \"Q5\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[:, column_index]  # Access a single column by integer index\n",
    "#df.iloc[:, [col_index1, col_index2]]  # Access multiple columns by integer indexes\n",
    "\n",
    "join_df.iloc[4:, [1, 2, 3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.iloc[6:, [1, 2, 3]] #Access columns using integer-based positions[row_first, columns_second]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_col_with_string(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"We want to traverse theough the df, and check for str, then print the column\n",
    "\n",
    "    Args:\n",
    "        df: The main tabular dataset\n",
    "\n",
    "    Returns:\n",
    "        The column that contain str dtype.\n",
    "    \"\"\"\n",
    "    col_with_string_dtype = join_df.select_dtypes(include=[\"object\"]).columns\n",
    "    for _, row in join_df.iterrows():\n",
    "        for col in col_with_string_dtype:\n",
    "            print(col)\n",
    "            if isinstance(row[col], str):\n",
    "                #print(f\"print {col} with the {row[col]}\")\n",
    "                return (row[col])\n",
    "\n",
    "print_col_with_string(join_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric data, the result’s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median.\n",
    "\n",
    "For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value’s frequency. Timestamps also include the first and last items.\n",
    "\n",
    "If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count.\n",
    "\n",
    "For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type.\n",
    "\n",
    "The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.describe(exclude=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a program that create a new dataframe with nan values.\n",
    "import numpy as np\n",
    "def create_new_df_with_nan(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"The function would traverse through the whole dataset, replace and fillna with different techniques used ou there\n",
    "\n",
    "    Args:\n",
    "        data: dataset\n",
    "\n",
    "    Returns:\n",
    "        cleaned df.\n",
    "    \"\"\"\n",
    "    new_dataset = {\"Q6\": [234, np.nan, 879, np.nan, 56.6, 765.0,876.4, 234.7, np.nan, 456.9,342,908, 432,765, 123.6, np.nan, 43.6, \"bigger\", \"larger\"], \"Q7\": [\"chucky\", 567, np.nan, 4, 56.6,765.0, np.nan, 234.7,98.5,456.9, np.nan, \"higher\", np.nan, 765, 123.6, 798.9, 43.6, \"lower\", \"tiny\"]}\n",
    "    df3 = pd.DataFrame(new_dataset)\n",
    "    df3 = pd.concat([join_df, df3], axis = 1)\n",
    "    df3['Q1'] = df3['Q1'].replace(167.0, 100)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = create_new_df_with_nan(join_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling nan values or missing values in dataframe.\n",
    "\n",
    "Filling NaN values in a DataFrame is a crucial step in data preprocessing to ensure that your dataset is complete and ready for analysis. Pandas provides various methods to handle missing values, both for single and multiple approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_copy = df3.copy()\n",
    "df3_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill with a Constant Value\n",
    "#You can fill all NaN values with a single constant value using fillna()\n",
    "\n",
    "df_filled = df3_copy.fillna(0)\n",
    "display(df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Fill\n",
    "\n",
    "df_filled = df3_copy.fillna(method='ffill')\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill NaN values\n",
    "df_filled = df3_copy.fillna(method='bfill')\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill with the Mean, Median, or Mode\n",
    "#You can fill NaN values with statistical values computed from the column.\n",
    "\n",
    "def fillna_with_M3(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filling nan values with mean, mean and mode\n",
    "\n",
    "    Args:\n",
    "        data: The main dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df_filled1 = df3.fillna(df3.mean())\n",
    "    df_filled2 = df3.fillna(df3.mode().iloc[0])\n",
    "\n",
    "    return df_filled1, df_filled2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled1, df_filled2 =  fillna_with_M3(df3)\n",
    "display(df_filled1)\n",
    "#print(\"\\n\")\n",
    "#print(df_filled2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Manipulation\n",
    "\n",
    "Create a new column based on operations performed on existing columns (e.g., summing two columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Q8\"] = df3[\"Q1\"] + df3[\"Q2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Q9\"] = df3[\"Q2\"] + df3[\"Q3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Q10\"] = df3[\"Q3\"] - df3[\"Q2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: \n",
    "Group data by a categorical column and calculate the average of a numerical column for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = df3.select_dtypes(include=\"object\")\n",
    "categorical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grougpy_by_category = df3.groupby([\"metrics\", \"Q5\", \"Q4\", \"Q6\", \"Q7\"]).mean()\n",
    "display(grougpy_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df3.fillna(method='ffill')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_info = {\"Email\": ['alice@example.com','charlie@samples.com', 'charlie@yahoo.com', 'bob@example.com','charlie@gamil.com', 'charlie@sample.com', 'david@website.com', 'edward@domain.com', 'edward@domain.com', 'edward@domain.com', 'david@website.canada', 'charle@yahoo.com', 'bob@example.eu', 'david@website.ca', 'charlie@sample.eu', 'charlie@sample.pl', 'alice@example.it', 'alice@example.uk', 'alice@example.cr'], \"annual_revenue\": [255.6, 367.7, 2443, 256.6, 678.5, 456.4, 865.43\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                              ,126.7,985.5, 675.4, 6746, 456.3, 6548, 567.4,2345,678,234.6,981.5, 567.8]}\n",
    "email_data = pd.DataFrame(other_info)\n",
    "email_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"Email\", \"Email\", \"Email\", \"Email\", \"annual_revenue\", \"annual_revenue\"]\n",
    "new_df = new_df.drop(columns=columns_to_drop, axis=1)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([new_df, email_data], axis=1)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expression, is a sequence of characters that forms a search pattern\n",
    "\n",
    "RegEx can be used to check if a string contains the specified search pattern.\n",
    "\n",
    "Regular expressions (regex) are a powerful tool for string matching and manipulation. In pandas, you can use regex to filter DataFrame rows based on column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where Email contains 'example'\n",
    "filtered_df = new_df[new_df['Email'].str.contains('.com', regex=True)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Encoding\n",
    "\n",
    "Converting categorical columns to numerical. If all dataset are int, float, you dont need to encode at all.\n",
    "\n",
    "1. One-Hot Encoding: How it works: Creates a new binary column for each unique category in the feature. A '1' indicates the presence of that category, and '0' indicates its absence.\n",
    "Example: If you have a \"Color\" feature with categories \"Red\", \"Green\", and \"Blue\", you'd get three new columns: \"Color_Red\", \"Color_Green\", \"Color_Blue\".\n",
    "Pros: Simple to understand and implement. Preserves information about all categories.\n",
    "Cons: Can lead to high dimensionality (lots of new columns) if you have many categories, potentially increasing computational cost and risk of overfitting.\n",
    "\n",
    "\n",
    "2. Label Encoding: How it works: Assigns a unique integer to each category.\n",
    "Example: \"Red\" might be 0, \"Green\" 1, and \"Blue\" 2.\n",
    "Pros: Reduces dimensionality compared to one-hot encoding.\n",
    "Cons: Introduces an artificial order among categories, which might not be meaningful. Can mislead some algorithms that assume numerical order.\n",
    "\n",
    "\n",
    "3. Ordinal Encoding: How it works: Assigns integers to categories based on a natural order or ranking.\n",
    "Example: \"Small\", \"Medium\", \"Large\" could be encoded as 1, 2, 3 respectively.\n",
    "Pros: Preserves some ordinal information if it exists.\n",
    "Cons: Only suitable for categories with a clear order. Can be subjective if the order is not well-defined.\n",
    "\n",
    "\n",
    "4. Target Encoding: How it works: Replaces each category with the mean (or other aggregate) of the target variable for that category.\n",
    "Example: If \"Color\" is encoded, each color would be replaced with the average target value for that color in the training data.\n",
    "Pros: Can capture complex relationships between categories and the target.\n",
    "Cons: Prone to overfitting if not used carefully. Can leak information from the target variable into the encoding.\n",
    "\n",
    "\n",
    "5. Hashing Trick: How it works: Maps categories to a fixed-size integer vector using a hash function.\n",
    "Pros: Handles high cardinality (many categories) efficiently.\n",
    "Cons: Can lose information about category relationships.\n",
    "\n",
    "Note: The best encoding method depends on:\n",
    "The nature of your categorical features: Are there clear orders or relationships?\n",
    "The size of your dataset: One-hot encoding can be problematic with many categories.\n",
    "The machine learning algorithm you're using: Some algorithms are more sensitive to encoding choices than others.\n",
    "\n",
    "Note that label encoder requires the input to be of a uniform type, typically strings or numbers, but not both. To solve it, convert Mixed Data Types to Strings: Ensure that all values in categorical columns are converted to strings, if that is what is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import List\n",
    "\n",
    "def convert_categorical_colu(data: pd.DataFrame) -> pd.DataFrame: \n",
    "    categorical_labels:  List = ['Email', 'Q5', 'Q4', 'Q6', 'Q7', 'metrics']\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in categorical_col:\n",
    "            if col in df.columns:\n",
    "                new_df[col] = label_encoder.fit_transform(data[col])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = convert_categorical_colu(new_df)\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically search through the df, and select the dtype with categorical col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "##from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#def label_encode_dynamic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "   # Label = LabelEncoder()\n",
    "    #categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "   # for col in categorical_cols:\n",
    "      #  df[col] = Label.fit_transform(df[col])\n",
    "    \n",
    "  #  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = label_encode_dynamic(new_df)\n",
    "#display(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
